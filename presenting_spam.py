# -*- coding: utf-8 -*-
"""presenting  spam.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XhtLPINGg_K4w3lxnZWoX_ii5HliTbrq
"""

from googleapiclient.discovery import build
import pandas as pd

api_key = 'AIzaSyDAnjuACJ6yDSzOzflAsVClbtJPkFINbfs'

def video_comments(video_id):
    comments_data = {
        'Comment': [],
        'Replies': []
    }

    youtube = build('youtube', 'v3', developerKey=api_key)
    video_response = youtube.commentThreads().list(
        part='snippet,replies',
        videoId=video_id
    ).execute()

    while video_response:
        for item in video_response['items']:
            comment = item['snippet']['topLevelComment']['snippet']['textDisplay']
            replies = []

            replycount = item['snippet']['totalReplyCount']
            if replycount > 0:
                for reply in item['replies']['comments']:
                    replies.append(reply['snippet']['textDisplay'])

            comments_data['Comment'].append(comment)
            comments_data['Replies'].append('; '.join(replies))

        if 'nextPageToken' in video_response:
            video_response = youtube.commentThreads().list(
                part='snippet,replies',
                videoId=video_id,
                pageToken=video_response['nextPageToken']
            ).execute()
        else:
            break

    return comments_data

video_id = "itwfmr4oqr0"

comments_data = video_comments(video_id)

comments_df = pd.DataFrame(comments_data)

csv_filename = 'youtube_comments.csv'
comments_df.to_csv(csv_filename, index=False, encoding='utf-8')

print(f"Comments data has been saved to '{csv_filename}'")

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
import os

from sklearn.preprocessing import LabelEncoder
from sklearn.utils import resample
from sklearn.feature_extraction.text import CountVectorizer
from nltk.sentiment.vader import SentimentIntensityAnalyzer

from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.stem import PorterStemmer, LancasterStemmer
from nltk.stem.snowball import SnowballStemmer
from nltk.corpus import stopwords
from nltk.corpus import wordnet
import string
from string import punctuation
import nltk
import re

print('youtube_comments.csv')
data = pd.read_csv(csv_filename)
print(data)

import pandas as pd
csv_filename = 'youtube_comments.csv'
data = pd.read_csv(csv_filename)

print(data.columns)

columns_to_drop = ['Unnamed: 0', 'Likes', 'Time', 'user', 'UserLink']

columns_to_drop_existing = [col for col in columns_to_drop if col in data.columns]
data1 = data.drop(columns_to_drop_existing, axis=1)

print(data1)

nltk.download('vader_lexicon')
sentiments = SentimentIntensityAnalyzer()
data1["Positive"] = [sentiments.polarity_scores(i)["pos"] for i in data1["Comment"]]
data1["Negative"] = [sentiments.polarity_scores(i)["neg"] for i in data1["Comment"]]
data1["Neutral"] = [sentiments.polarity_scores(i)["neu"] for i in data1["Comment"]]
data1['Compound'] = [sentiments.polarity_scores(i)["compound"] for i in data1["Comment"]]
score = data1["Compound"].values
sentiment = []
for i in score:
    if i >= 0.05 :
        sentiment.append('Positive')
    elif i <= -0.05 :
        sentiment.append('Negative')
    else:
        sentiment.append('Neutral')

data1["Sentiment"] = sentiment
data1.head()

data2=data1.drop(['Positive','Negative','Neutral','Compound'],axis=1)
data2.head()

import nltk
nltk.download('stopwords')

stop_words = stopwords.words('english')
porter_stemmer = PorterStemmer()
lancaster_stemmer = LancasterStemmer()
snowball_stemer = SnowballStemmer(language="english")
lzr = WordNetLemmatizer()

def text_processing(text):
    text = text.lower()
    text = re.sub(r'\n',' ', text)
    text = re.sub('[%s]' % re.escape(punctuation), "", text)
    text = re.sub("^a-zA-Z0-9$,.", "", text)
    text = re.sub(r'\s+', ' ', text, flags=re.I)
    text = re.sub(r'\W', ' ', text)
    text = ' '.join([word for word in word_tokenize(text) if word not in stop_words])
    text=' '.join([lzr.lemmatize(word) for word in word_tokenize(text)])
    return text

import nltk

nltk.download("wordnet")
nltk.corpus.wordnet.synsets("test")

import nltk
nltk.download('punkt')
nltk.download('omw-1.4')
data_copy = data2.copy()
data_copy.Comment = data_copy.Comment.apply(lambda text: text_processing(text))

le = LabelEncoder()
data_copy['Sentiment'] = le.fit_transform(data_copy['Sentiment'])

processed_data = {
    'Sentence':data_copy.Comment,
    'Sentiment':data_copy['Sentiment']
}

processed_data = pd.DataFrame(processed_data)
processed_data.head()

processed_data['Sentiment'].value_counts()

df_neutral = processed_data[(processed_data['Sentiment']==1)]
df_negative = processed_data[(processed_data['Sentiment']==0)]
df_positive = processed_data[(processed_data['Sentiment']==2)]

df_negative_upsampled = resample(df_negative,
                                 replace=True,
                                 n_samples= 205,
                                 random_state=42)

df_neutral_upsampled = resample(df_neutral,
                                 replace=True,
                                 n_samples= 205,
                                 random_state=42)

final_data = pd.concat([df_negative_upsampled,df_neutral_upsampled,df_positive])

final_data['Sentiment'].value_counts()

import pandas as pd
from sklearn.utils import resample
import matplotlib.pyplot as plt

sentiment_counts = final_data['Sentiment'].value_counts().sort_index()

plt.figure(figsize=(8, 6))
plt.plot(sentiment_counts.index, sentiment_counts.values, marker='o', linestyle='-', color='b')
plt.title('Sentiment Distribution')
plt.xlabel('Sentiment')
plt.ylabel('Count')
plt.xticks(sentiment_counts.index)
plt.grid(True)
plt.show()

sentiment_counts = final_data['Sentiment'].value_counts().sort_index()

colors = ['red', 'blue', 'purple']

plt.figure(figsize=(8, 6))
sentiment_counts.plot(kind='bar', color=colors)
plt.title('Sentiment Distribution')
plt.xlabel('Sentiment')
plt.ylabel('Count')
plt.xticks(rotation=0)
plt.grid(axis='y')
plt.show()

corpus = []
for sentence in final_data['Sentence']:
    corpus.append(sentence)
corpus[0:5]

from sklearn.feature_extraction.text import CountVectorizer
cv = CountVectorizer(max_features=1500)
X = cv.fit_transform(corpus).toarray()
y = final_data.iloc[:, -1].values

from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)
classifier = GaussianNB()
classifier.fit(X_train, y_train)

from sklearn.metrics import confusion_matrix, accuracy_score
y_pred = classifier.predict(X_test)
cm = confusion_matrix(y_test, y_pred)
cm

nb_score = accuracy_score(y_test, y_pred)
print('accuracy',nb_score)

from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

svm_classifier = SVC(kernel='linear')
svm_classifier.fit(X_train, y_train)

y_pred_svm = svm_classifier.predict(X_test)

cm_svm = confusion_matrix(y_test, y_pred_svm)
print("Support Vector Machines (SVM):\n", cm_svm)

svm_score = accuracy_score(y_test, y_pred_svm)
print('Accuracy (SVM):', svm_score)

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

logistic_classifier = LogisticRegression()

logistic_classifier.fit(X_train, y_train)

y_pred_logistic = logistic_classifier.predict(X_test)

cm_logistic = confusion_matrix(y_test, y_pred_logistic)
print("Logistic Regression:\n", cm_logistic)

logistic_score = accuracy_score(y_test, y_pred_logistic)
print('Accuracy (Logistic Regression):', logistic_score)

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.model_selection import train_test_split

random_forest_classifier = RandomForestClassifier(n_estimators=100, random_state=0)

random_forest_classifier.fit(X_train, y_train)

y_pred_rf = random_forest_classifier.predict(X_test)

cm_rf = confusion_matrix(y_test, y_pred_rf)
print("Random Forest Confusion Matrix:\n", cm_rf)

rf_score = accuracy_score(y_test, y_pred_rf)
print('Accuracy (Random Forest):', rf_score)

import matplotlib.pyplot as plt

algorithms = ['Naive Bayes', 'Support Vector Machines', 'Logistic Regression', 'Random Forest']
accuracy_scores = [nb_score, svm_score, logistic_score, rf_score]
colors = ['skyblue', 'salmon', 'lightgreen', 'orange']

plt.figure(figsize=(10, 6))  # Increased figure size
bar_plot = plt.bar(algorithms, accuracy_scores, color=colors, width=0.6)  # Adjusted bar width

plt.title('Accuracy Comparison of Different Algorithms\n\n', fontsize=16)  # Increased title font size
plt.xlabel('\nAlgorithms', fontsize=12)  # Increased x-axis label font size
plt.ylabel('Accuracy', fontsize=12)  # Increased y-axis label font size
plt.ylim(0, 1)
plt.grid(axis='y')

for i, score in enumerate(accuracy_scores):
    plt.text(i, score + 0.02, f'{score:.2f}', ha='center', va='bottom', fontsize=10)  # Adjusted text font size

plt.tight_layout()  # Improves spacing to prevent label cutoff
plt.show()

from sklearn.ensemble import StackingClassifier, BaggingClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import train_test_split
logreg = LogisticRegression(C=1, max_iter = 2000)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)


svm_model = SVC(probability=True)
logreg_model = LogisticRegression()
rf_model = RandomForestClassifier(n_estimators=500)
nb_model = GaussianNB()

estimators_stack = [('svm', svm_model), ('logreg', logreg_model), ('rf', rf_model), ('nb', nb_model)]
stacking_clf = StackingClassifier(
    estimators=estimators_stack,
    final_estimator=LogisticRegression()
)

stacking_clf.fit(X_train, y_train)

accuracy_stack = stacking_clf.score(X_test, y_test)
print("Stacking Classifier Accuracy:", accuracy_stack)

bagging_clf = BaggingClassifier(
    base_estimator=LogisticRegression(),
    n_estimators=10,
    random_state=0
)

bagging_clf.fit(X_train, y_train)

accuracy_bagging = bagging_clf.score(X_test, y_test)
print("Bagging Classifier Accuracy:", accuracy_bagging)

import matplotlib.pyplot as plt

classifiers = ['Stacking Classifier', 'Bagging Classifier']
accuracy_scores = [accuracy_stack, accuracy_bagging]

plt.figure(figsize=(5,6))
plt.bar(classifiers, accuracy_scores, color=['skyblue', 'salmon'])
plt.title('Accuracy Comparison: Stacking vs Bagging Classifiers \n\n', fontsize= 12)
plt.xlabel('\nClassifiers')
plt.ylabel('Accuracy')
plt.ylim(0, 1)
plt.grid(axis='y')

for i, score in enumerate(accuracy_scores):
    plt.text(i, score + 0.02, f'{score:.2f}', ha='center', va='bottom')

plt.show()

import pandas as pd
import zipfile
import pickle

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer

from sklearn.naive_bayes import  MultinomialNB

from sklearn.metrics import confusion_matrix, classification_report

from google.colab import files
uploaded = files.upload()

dataset_df=pd.read_csv("dataset.csv")
dataset_df.head()

dataset_df.info()

X_train, X_test, y_train, y_test = train_test_split(dataset_df["CONTENT"], dataset_df["CLASS"])

tfidf_vect = TfidfVectorizer(use_idf=True, lowercase=True)
X_train_tfidf = tfidf_vect.fit_transform(X_train)
X_train_tfidf.shape

model = MultinomialNB()
model.fit(X_train_tfidf, y_train)

X_test_tfidf = tfidf_vect.transform(X_test)
predictions = model.predict(X_test_tfidf)

confusion_matrix(y_test, predictions)

model.score(X_test_tfidf, y_test)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix


tfidf_vect = TfidfVectorizer(use_idf=True, lowercase=True)
X_train_tfidf = tfidf_vect.fit_transform(X_train)
X_test_tfidf = tfidf_vect.transform(X_test)

svm_model = SVC(kernel='linear')
svm_model.fit(X_train_tfidf, y_train)
svm_predictions = svm_model.predict(X_test_tfidf)

svm_confusion = confusion_matrix(y_test, svm_predictions)
print("Confusion Matrix for SVM Classifier:")
print(svm_confusion)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix, classification_report

tfidf_vect = TfidfVectorizer(use_idf=True, lowercase=True)

X_train_tfidf = tfidf_vect.fit_transform(X_train)
X_test_tfidf = tfidf_vect.transform(X_test)

svm_model = SVC(kernel='linear')

svm_model.fit(X_train_tfidf, y_train)

svm_predictions = svm_model.predict(X_test_tfidf)

svm_accuracy = svm_model.score(X_test_tfidf, y_test)
print(f"Accuracy of SVM Classifier: {svm_accuracy:.2f}")

svm_confusion = confusion_matrix(y_test, svm_predictions)
svm_classification = classification_report(y_test, svm_predictions)
print("Confusion Matrix for SVM Classifier:")
print(svm_confusion)
print("Classification Report for SVM Classifier:")
print(svm_classification)

import pandas as pd
youtube_comments_file = 'youtube_comments.csv'
youtube_data = pd.read_csv(youtube_comments_file)

youtube_comments = youtube_data['Comment'].tolist()
processed_youtube_comments = [text_processing(comment) for comment in youtube_comments]

X_youtube_tfidf = tfidf_vect.transform(processed_youtube_comments)

predictions_youtube = model.predict(X_youtube_tfidf)

for idx, (comment, prediction) in enumerate(zip(youtube_comments, predictions_youtube)):
    print(f"Comment {idx + 1}:\n{'-' * 20}\n{comment}\n{'-' * 20}")
    print(f"Prediction: {'Spam' if prediction == 1 else 'Not Spam'}")
    print('\n')

import pandas as pd
import matplotlib.pyplot as plt


youtube_comments_file = 'youtube_comments.csv'
youtube_data = pd.read_csv(youtube_comments_file)

youtube_comments = youtube_data['Comment'].tolist()

processed_youtube_comments = [text_processing(comment) for comment in youtube_comments]

X_youtube_tfidf = tfidf_vect.transform(processed_youtube_comments)

predictions_youtube = model.predict(X_youtube_tfidf)

youtube_data['Predictions'] = predictions_youtube

print(youtube_data[['Comment', 'Predictions']])
prediction_counts = youtube_data['Predictions'].value_counts()

plt.figure(figsize=(8, 6))
prediction_counts.plot(kind='bar', color=['skyblue', 'orange'])
plt.title('Distribution of Predicted Labels (Spam vs. Not Spam)')
plt.xlabel('Predicted Labels')
plt.ylabel('Number of Comments')
plt.xticks(rotation=0)
plt.show()

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report

youtube_comments_file = 'youtube_comments.csv'
youtube_data = pd.read_csv(youtube_comments_file)

youtube_comments = youtube_data['Comment']
X_youtube_tfidf = tfidf_vect.transform(youtube_comments)

predictions_youtube = model.predict(X_youtube_tfidf)

different_labeled_data = pd.read_csv('dataset.csv')
different_labels = different_labeled_data['CONTENT']

actual_labels_different_data = different_labeled_data['CLASS']

X_different_labels_tfidf = tfidf_vect.transform(different_labels)

predictions_different_labels = model.predict(X_different_labels_tfidf)

accuracy = accuracy_score(actual_labels_different_data, predictions_different_labels)
print(f"Accuracy on Different Labeled Dataset: {accuracy:.2f}")

classification_rep = classification_report(actual_labels_different_data, predictions_different_labels)
print("Classification Report on Different Labeled Dataset:")
print(classification_rep)